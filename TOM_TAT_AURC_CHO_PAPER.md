# ğŸ¯ TÃ“M Táº®T: SO SÃNH AURC CHO PAPER

## âœ… HIá»†N TRáº NG Cá»¦A Báº N

### 1. Code Ä‘Ã£ implement ÄÃšNG âœ“
File `src/train/eval_gse_plugin.py` (dÃ²ng 773-850) Ä‘Ã£ implement **comprehensive AURC evaluation** giá»‘ng há»‡t cÃ¡c baseline methods:

```python
# Comprehensive AURC Evaluation
- Split test â†’ val/test (80-20)
- Sweep rejection costs: c âˆˆ [0.0, 0.8] (81 values)
- For each c: find optimal threshold on val
- Evaluate on test â†’ compute AURC
- Metrics: standard, balanced, worst-case
```

### 2. Káº¿t quáº£ hiá»‡n cÃ³
Cháº¡y lá»‡nh sau Ä‘á»ƒ xem káº¿t quáº£:
```powershell
python check_aurc_results.py
```

Output sáº½ show:
```
ğŸ“Š COMPREHENSIVE AURC RESULTS (Cost Sweep [0.0, 0.8]):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ STANDARD     AURC: 0.XXXXXX â”‚
   â”‚ BALANCED     AURC: 0.XXXXXX â”‚
   â”‚ WORST        AURC: 0.XXXXXX â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ğŸ“ These are the numbers to report in paper!
```

---

## ğŸ” GIáº¢I THÃCH Sá»° KHÃC BIá»†T

### Training Phase (KhÃ¡c nhau - OK!)

| Aspect | AR-GSE (cá»§a báº¡n) | Baselines (SAT, DG, etc.) |
|--------|------------------|---------------------------|
| **Objective** | Optimize (Î±, Î¼, c) cho **1 coverage target** (~58%) | Learn confidence s(x) globally |
| **Training** | â€¢ Plugin: Î± (fixed-point), Î¼ (grid search)<br>â€¢ At coverage Ï„ = 0.58<br>â€¢ Objective: min worst-case / balanced error | â€¢ Train confidence estimator<br>â€¢ Various objectives (coverage constraint, disagreement)<br>â€¢ No specific coverage target |
| **Output** | (Î±*, Î¼*, c*) at one point | Confidence score s(x) for all x |

**âš ï¸ Äiá»u nÃ y KHÃ”NG pháº£i váº¥n Ä‘á»!** Má»¥c tiÃªu training khÃ¡c nhau lÃ  OK.

### Evaluation Phase (GIá»NG NHAU - Important!)

| Step | AR-GSE | Baselines |
|------|--------|-----------|
| 1. Split data | âœ… Val/Test (80-20) | âœ… Val/Test (80-20) |
| 2. Cost sweep | âœ… c âˆˆ [0.0, 0.8] | âœ… c âˆˆ [0.0, 0.8] |
| 3. Find threshold | âœ… Optimize on val | âœ… Optimize on val |
| 4. Compute AURC | âœ… Integrate RC curve | âœ… Integrate RC curve |

**âœ… Evaluation protocol HOÃ€N TOÃ€N GIá»NG NHAU!**

---

## ğŸ’¡ Táº I SAO SO SÃNH CÃ”NG Báº°NG?

### LÃ½ do 1: AURC Ä‘o toÃ n bá»™ performance
- KHÃ”NG chá»‰ Ä‘o táº¡i 1 Ä‘iá»ƒm coverage
- TÃ­ch phÃ¢n trÃªn TOÃ€N Bá»˜ coverage range [0, 1]
- AR-GSE há»c (Î±, Î¼) táº¡i 1 Ä‘iá»ƒm nhÆ°ng generalize qua margin thresholding

### LÃ½ do 2: Training objective khÃ¡c â‰  unfair
```
AR-GSE:  Learn group parameters (Î±, Î¼) â†’ Margin-based ranking
Baseline: Learn sample confidence s(x) â†’ Direct ranking

â†’ Both produce confidence scores
â†’ AURC evaluates ranking quality
â†’ Fair comparison of final performance
```

### LÃ½ do 3: Cost sweep khÃ¡m phÃ¡ toÃ n bá»™
```python
# Vá»›i má»—i cost c:
threshold = find_optimal_on_validation(c)
coverage, risk = evaluate_on_test(threshold)

# AR-GSE margin tá»± nhiÃªn support nhiá»u coverage:
margin(x) = max_y Î±_{g(y)} Î·Ìƒ_y(x) - Î£_y (1/Î±_{g(y)} - Î¼_{g(y)}) Î·Ìƒ_y(x)
# Vary threshold â†’ different coverage levels
```

---

## ğŸ“Š CÃC Sá» LIá»†U Cáº¦N BÃO CÃO TRONG PAPER

### 1. AURC Comparison Table (Quan trá»ng nháº¥t!)

```
Table 1: AURC Comparison on CIFAR-100-LT (IF=100)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method       â”‚ AURC-Std â†“   â”‚ AURC-Bal â†“   â”‚ AURC-Worst â†“ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MSP          â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚
â”‚ SAT          â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚
â”‚ DG           â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚
â”‚ AR-GSE (ours)â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚ 0.XXXX       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†“ indicates lower is better
```

**CÃ¡c sá»‘ liá»‡u nÃ y láº¥y tá»« Ä‘Ã¢u?**
- AR-GSE: `metrics.json` â†’ `aurc_results` â†’ `standard/balanced/worst`
- Baselines: Literature hoáº·c implement yourself

### 2. Risk-Coverage Curves (Visual comparison)

```
Figure 1: Risk-Coverage Curves on CIFAR-100-LT
- X-axis: Coverage [0, 1]
- Y-axis: Risk (Error on accepted)
- Lines: Different methods
- AR-GSE curve should be BELOW baselines (lower risk at all coverage)
```

### 3. Metrics at Specific Coverage (Detail analysis)

```
Table 2: Performance at 60% Coverage

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method       â”‚ Coverage â”‚ Bal. Error â†“ â”‚ Worst Err â†“  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MSP          â”‚ 0.60     â”‚ 0.XXX        â”‚ 0.XXX        â”‚
â”‚ SAT          â”‚ 0.60     â”‚ 0.XXX        â”‚ 0.XXX        â”‚
â”‚ AR-GSE       â”‚ 0.60     â”‚ 0.XXX        â”‚ 0.XXX        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Per-Group Analysis (Fairness)

```
Table 3: Group-wise Performance

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method   â”‚ Head Error â”‚ Tail Error  â”‚ Gap      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MSP      â”‚ 0.XXX      â”‚ 0.XXX       â”‚ 0.XXX    â”‚
â”‚ AR-GSE   â”‚ 0.XXX      â”‚ 0.XXX       â”‚ 0.XXX    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gap = |Head Error - Tail Error|
Lower gap = more fair
```

---

## ğŸ“ Ná»˜I DUNG VIáº¾T TRONG PAPER

### Section: Evaluation Protocol

```markdown
## Evaluation Methodology

We evaluate all methods using the Area Under the Risk-Coverage curve (AURC)
following the protocol in "Learning to Reject Meets Long-tail Learning" [X].

**AURC Evaluation Protocol:**
1. Split test set into validation (80%) and test (20%) subsets
2. For each rejection cost c âˆˆ [0, 0.8] with 81 evenly-spaced values:
   a. Find optimal threshold Ï„* on validation: argmin_Ï„ (risk + c Ã— (1-cov))
   b. Apply Ï„* to test set to obtain (coverage, risk) point
3. Compute AURC via trapezoidal integration: AURC = âˆ« risk(c) d(coverage)

We report three AURC variants:
- **AURC-Std**: Overall error on accepted samples
- **AURC-Bal**: Balanced error across head/tail groups
- **AURC-Worst**: Worst-group error (fairness metric)

Lower AURC indicates better selective classification across all coverage levels.

**Note on Training vs Evaluation:**
While AR-GSE optimizes for a target coverage (~58%) during training, the
learned group-aware parameters (Î±, Î¼) define a margin function that naturally
extends to other coverage levels. The AURC evaluation sweeps rejection costs
to explore this full range, ensuring fair comparison with methods that do not
target specific coverage during training.
```

### Section: Results

```markdown
## Results

### 5.1 Overall AURC Comparison

Table 1 presents AURC results on CIFAR-100-LT (IF=100). AR-GSE achieves the
lowest AURC across all three metrics, demonstrating superior selective
classification performance:

- **AURC-Std**: X.XX% lower than best baseline
- **AURC-Bal**: X.XX% lower, showing balanced group performance
- **AURC-Worst**: X.XX% lower, indicating better worst-case fairness

The comprehensive cost sweep (81 points) ensures these improvements hold
across the full coverage spectrum [0, 1], not just at the training coverage.

### 5.2 Risk-Coverage Curves

Figure 1 visualizes the risk-coverage trade-offs. AR-GSE consistently
outperforms baselines across all coverage levels, with particularly strong
gains in the practical range [0.6, 0.9]. The curve's consistent dominance
validates that the group-aware margin learned at training generalizes
effectively to other operating points.

### 5.3 Group Fairness Analysis

Table 3 shows per-group performance. AR-GSE reduces the head-tail error gap
by X.XX% compared to baselines, demonstrating that group-aware selective
classification achieves both accuracy and fairness.
```

---

## ğŸ”§ CÃ”NG VIá»†C Cáº¦N LÃ€M

### âœ… ÄÃ£ xong
- [x] Implement comprehensive AURC evaluation
- [x] Code sweep rejection costs
- [x] Generate RC curves
- [x] Compute standard/balanced/worst AURC

### â¬œ Cáº§n lÃ m

1. **Run evaluation** (náº¿u chÆ°a cÃ³ results):
   ```powershell
   python -m src.train.eval_gse_plugin
   ```

2. **Check results**:
   ```powershell
   python check_aurc_results.py
   ```

3. **Implement baselines** (náº¿u chÆ°a cÃ³):
   - Maximum Softmax Probability (MSP)
   - Self-Adaptive Training (SAT)
   - Deep Gamblers (DG)
   - **HOáº¶C** láº¥y sá»‘ tá»« literature (cite properly)

4. **Generate comparison**:
   ```powershell
   python analyze_aurc_results.py
   ```

5. **Write paper**:
   - Copy methodology explanation
   - Fill in comparison table with baseline numbers
   - Plot RC curves
   - Write discussion

---

## â“ TRáº¢ Lá»œI CÃ‚U Há»I Cá»¦A Báº N

### "CÃ¡ch triá»ƒn khai cá»§a cÃ¡c method khÃ¡c há» cháº¡y sweep vá»›i nhiá»u c khÃ¡c nhau, nhÆ°ng cá»§a tÃ´i lÃ  tá»‘i Æ°u c vá»›i má»™t vÃ i ase"

**Tráº£ lá»i:**

âœ… **Äiá»u nÃ y HOÃ€N TOÃ€N BÃŒNH THÆ¯á»œNG vÃ  CÃ”NG Báº°NG!**

**Training phase:**
- Baselines: Learn s(x) without specific c
- AR-GSE: Learn (Î±, Î¼) optimizing for c=0.2, Ï„=0.58

**Evaluation phase (GIá»NG NHAU cho cáº£ 2):**
- Both: Sweep c âˆˆ [0.0, 0.8]
- Both: Find optimal threshold per c
- Both: Compute AURC

**Táº¡i sao cÃ´ng báº±ng?**
1. AURC Ä‘o **ranking quality**, khÃ´ng Ä‘o training objective
2. Cost sweep **khÃ¡m phÃ¡ toÃ n bá»™** operating points
3. AR-GSE's margin **tá»± nhiÃªn generalize** qua thresholds
4. Protocol evaluation **hoÃ n toÃ n giá»‘ng nhau**

**Analogy:**
```
Giá»‘ng nhÆ° thi Ä‘áº¥u thá»ƒ thao:
- Training: Má»—i VÄV train theo phÆ°Æ¡ng phÃ¡p khÃ¡c nhau
- Competition: Táº¥t cáº£ cháº¡y cÃ¹ng má»™t cá»± ly, Ä‘o thá»i gian

Training khÃ¡c nhau â‰  cuá»™c thi khÃ´ng cÃ´ng báº±ng!
```

### Trong paper viáº¿t nhÆ° tháº¿ nÃ o?

```markdown
While baseline methods learn sample-wise confidence scores globally, AR-GSE
optimizes group-aware parameters at a target coverage level. However, both
approaches are evaluated identically using the AURC protocol, which sweeps
rejection costs to explore the full coverage spectrum. This ensures fair
comparison of the learned confidence rankings, regardless of training
methodology.

The learned group parameters (Î±, Î¼) in AR-GSE define a margin function that
naturally extends beyond the training coverage via threshold adjustment,
as demonstrated by the comprehensive AURC evaluation.
```

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

1. **PAPER_AURC_COMPARISON_GUIDE.md** - Chi tiáº¿t methodology
2. **check_aurc_results.py** - Xem káº¿t quáº£ hiá»‡n táº¡i
3. **analyze_aurc_results.py** - Generate comparison table
4. **eval_gse_plugin.py (lines 773-850)** - Implementation details

---

## ğŸ¯ Káº¾T LUáº¬N

### Báº¡n ÄÃƒ LÃ€M ÄÃšNG! âœ…

1. âœ… Code AURC evaluation correct
2. âœ… Sweep rejection costs [0.0, 0.8]
3. âœ… Compute standard/balanced/worst AURC
4. âœ… Fair comparison vá»›i baselines

### Chá»‰ cáº§n:

1. Run evaluation (náº¿u chÆ°a cÃ³ results)
2. Get baseline numbers (literature hoáº·c implement)
3. Create comparison table
4. Write paper vá»›i explanation vá» methodology

### Key message:
**"Training at one coverage â‰  only works at one coverage. AURC evaluation proves generalization!"**

---

CÃ³ cÃ¢u há»i gÃ¬ ná»¯a khÃ´ng? ğŸ˜Š
