# EVALUATION_GUIDE.md

# HÆ°á»›ng Dáº«n Evaluation So SÃ¡nh vá»›i Paper Benchmarks

TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n cÃ¡ch Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh AR-GSE vÃ  so sÃ¡nh vá»›i cÃ¡c paper nhÆ° **SelectiveNet** vÃ  **Learning to Reject Meets Long-tail Learning**.

---

## ğŸ“‹ Tá»•ng Quan

ChÃºng tÃ´i cung cáº¥p 3 script evaluation:

1. **`eval_comprehensive.py`** - Evaluation toÃ n diá»‡n vá»›i nhiá»u phÆ°Æ¡ng phÃ¡p tÃ­nh confidence
2. **`eval_paper_benchmark.py`** - So sÃ¡nh trá»±c tiáº¿p vá»›i paper benchmarks  
3. **`run_benchmark_evaluation.py`** - Script nhanh Ä‘á»ƒ cháº¡y evaluation

---

## ğŸ¯ Metrics ChÃ­nh

### 1. AURC (Area Under Risk-Coverage Curve)
- **Äá»‹nh nghÄ©a**: Diá»‡n tÃ­ch dÆ°á»›i Ä‘Æ°á»ng cong Risk-Coverage
- **CÃ´ng thá»©c**: $\text{AURC} = \int_{c_{min}}^{c_{max}} \text{Risk}(c) \, dc$
- **Tá»‘t hÆ¡n khi**: Tháº¥p hÆ¡n (lower is better)
- **Range**: [0, 1]

### 2. E-AURC (Excess AURC)
- **Äá»‹nh nghÄ©a**: AURC vÆ°á»£t quÃ¡ baseline ngáº«u nhiÃªn
- **CÃ´ng thá»©c**: $\text{E-AURC} = \text{AURC}_{\text{method}} - \text{AURC}_{\text{random}}$
- **Tá»‘t hÆ¡n khi**: Ã‚m (negative is better)

### 3. Coverage-Error Trade-offs
- ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t táº¡i cÃ¡c má»©c coverage cá»¥ thá»ƒ: 60%, 70%, 80%, 90%, 95%
- So sÃ¡nh Standard Error, Balanced Error, Worst-Group Error

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### PhÆ°Æ¡ng Ã¡n 1: Script nhanh (Khuyáº¿n nghá»‹)

```bash
# Evaluation cÆ¡ báº£n
python run_benchmark_evaluation.py

# Vá»›i custom checkpoint
python run_benchmark_evaluation.py \
    --checkpoint ./checkpoints/my_model/gse_plugin.ckpt \
    --output ./my_results

# Chá»‰ Ä‘á»‹nh dataset vÃ  experts
python run_benchmark_evaluation.py \
    --checkpoint ./checkpoints/argse_balanced/cifar100_lt_if100/gse_plugin.ckpt \
    --dataset cifar100_lt_if100 \
    --experts ce_baseline logitadjust_baseline balsoftmax_baseline \
    --output ./benchmark_results_balanced
```

### PhÆ°Æ¡ng Ã¡n 2: Python script chi tiáº¿t

```python
from src.train.eval_paper_benchmark import PaperBenchmarkEvaluator

config = {
    'dataset': {
        'name': 'cifar100_lt_if100',
        'splits_dir': './data/cifar100_lt_if100_splits',
        'num_classes': 100,
    },
    'experts': {
        'names': ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline'],
        'logits_dir': './outputs/logits',
    },
    'checkpoint_path': './checkpoints/argse_worst_eg_improved/cifar100_lt_if100/gse_balanced_plugin.ckpt',
    'output_dir': './paper_benchmark_results',
    'seed': 42
}

evaluator = PaperBenchmarkEvaluator(config)
results = evaluator.run_paper_benchmark()
```

### PhÆ°Æ¡ng Ã¡n 3: Comprehensive evaluation (so sÃ¡nh nhiá»u methods)

```python
from src.train.eval_comprehensive import ComprehensiveEvaluator

config = {
    'dataset': {
        'name': 'cifar100_lt_if100',
        'splits_dir': './data/cifar100_lt_if100_splits',
        'num_classes': 100,
    },
    'experts': {
        'names': ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline'],
        'logits_dir': './outputs/logits',
    },
    'checkpoint_path': './checkpoints/argse_worst_eg_improved/cifar100_lt_if100/gse_balanced_plugin.ckpt',
    'output_dir': './comprehensive_evaluation_results',
    'coverage_min': 0.2,
    'coverage_max': 1.0,
    'num_points': 81,
    'seed': 42
}

evaluator = ComprehensiveEvaluator(config)
results = evaluator.run_full_evaluation()
```

---

## ğŸ“Š Output Files

Sau khi cháº¡y evaluation, báº¡n sáº½ cÃ³ cÃ¡c file:

### 1. `paper_benchmark_results.json`
```json
{
    "dataset": "cifar100_lt_if100",
    "num_test_samples": 10000,
    "method": "gse_margin",
    "aurc_metrics": {
        "standard": {
            "aurc": 0.123456,
            "eaurc": -0.012345
        },
        "balanced": {
            "aurc": 0.234567,
            "eaurc": -0.023456
        },
        "worst": {
            "aurc": 0.345678,
            "eaurc": -0.034567
        }
    },
    "oracle": {
        "aurc_standard": 0.056789,
        "aurc_balanced": 0.067890,
        "aurc_worst": 0.078901
    },
    "coverage_metrics": {
        "cov_0.60": {
            "standard_accuracy": 0.85,
            "balanced_accuracy": 0.82,
            "worst_accuracy": 0.78
        }
    }
}
```

### 2. `paper_benchmark_figures.png/pdf`
6-panel figure cháº¥t lÆ°á»£ng publication:
- (a) Standard Error RC Curve
- (b) Balanced Error RC Curve  
- (c) Worst-Group Error RC Curve
- (d) Per-Group Selective Errors
- (e) AURC Metrics Comparison
- (f) Coverage-Accuracy Trade-off

### 3. `latex_table.tex`
Báº£ng LaTeX sáºµn sÃ ng cho paper submission:
```latex
\begin{table}[t]
\centering
\caption{Selective Classification Performance on CIFAR-100-LT}
\label{tab:main_results}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{AURC-Std} & \textbf{AURC-Bal} & \textbf{AURC-Worst} \\
\midrule
AR-GSE (Ours) & 0.1234 & 0.2345 & \textbf{0.3456} \\
...
\end{tabular}
\end{table}
```

### 4. `rc_curve_paper_benchmark.csv`
Raw data cá»§a RC curve Ä‘á»ƒ váº½ láº¡i hoáº·c phÃ¢n tÃ­ch thÃªm

---

## ğŸ”¬ Confidence Scoring Methods

Framework há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p tÃ­nh confidence score:

### 1. GSE Margin (Khuyáº¿n nghá»‹)
```python
confidence = compute_margin(eta_mix, alpha, mu, c=0.0, class_to_group)
```
- Sá»­ dá»¥ng GSE decision function: $\text{score} - \text{threshold}$
- PhÃ¹ há»£p nháº¥t vá»›i AR-GSE framework

### 2. GSE Raw Margin
```python
confidence = compute_raw_margin(eta_mix, alpha, mu, class_to_group)
```
- KhÃ´ng trá»« rejection cost $c$

### 3. Maximum Posterior
```python
confidence = max_y P(y|x)
```
- Standard baseline tá»« SelectiveNet

### 4. Entropy-based
```python
confidence = -H(P(y|x)) = -Î£_y P(y|x) log P(y|x)
```
- Negative entropy lÃ m confidence score

### 5. Margin (Top-2 difference)
```python
confidence = P(y_1|x) - P(y_2|x)
```
- Hiá»‡u sá»‘ giá»¯a 2 xÃ¡c suáº¥t cao nháº¥t

---

## ğŸ“ˆ CÃ¡ch Hiá»ƒu Káº¿t Quáº£

### âœ… Káº¿t quáº£ tá»‘t:
- **AURC tháº¥p** (< 0.3 lÃ  tá»‘t, < 0.2 lÃ  ráº¥t tá»‘t)
- **E-AURC Ã¢m** (cÃ ng Ã¢m cÃ ng tá»‘t so vá»›i random)
- **AURC gáº§n Oracle** (< 2x Oracle lÃ  acceptable)
- **Worst-Group Error khÃ´ng quÃ¡ cao** (< 2x Balanced Error)

### âš ï¸ Cáº£nh bÃ¡o:
- AURC > 0.5: Model khÃ´ng há»c Ä‘Æ°á»£c gÃ¬
- E-AURC > 0: Tá»‡ hÆ¡n random rejection
- Worst-Group Error >> Balanced Error: ThiÃªn lá»‡ch nghiÃªm trá»ng

### ğŸ“Š So sÃ¡nh vá»›i Papers:

#### SelectiveNet (ICML 2019)
- **CIFAR-10**: AURC â‰ˆ 0.02-0.03
- **CIFAR-100**: AURC â‰ˆ 0.10-0.15
- **ImageNet**: AURC â‰ˆ 0.05-0.08

#### Learning to Reject Meets Long-tail Learning (2024)
- **CIFAR-100-LT**: 
  - Balanced AURC â‰ˆ 0.15-0.20
  - Worst-Group AURC â‰ˆ 0.25-0.35
  - Coverage 0.7: Balanced Acc â‰ˆ 0.75-0.80

---

## ğŸ” Debug vÃ  Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

#### 1. File not found
```
FileNotFoundError: Logits file not found
```
**Giáº£i phÃ¡p**: Kiá»ƒm tra Ä‘Æ°á»ng dáº«n logits:
```bash
ls -la outputs/logits/cifar100_lt_if100/*/test_lt_logits.pt
```

#### 2. AURC quÃ¡ cao (> 0.5)
**NguyÃªn nhÃ¢n**: Model chÆ°a converge hoáº·c confidence scores khÃ´ng há»£p lÃ½

**Giáº£i phÃ¡p**:
- Kiá»ƒm tra confidence score distribution:
```python
import torch
confidence = evaluator.compute_confidence_scores('gse_margin')
print(f"Min: {confidence.min():.3f}, Max: {confidence.max():.3f}")
print(f"Mean: {confidence.mean():.3f}, Std: {confidence.std():.3f}")
```
- Thá»­ phÆ°Æ¡ng phÃ¡p khÃ¡c: `max_posterior`, `entropy`

#### 3. Worst > Balanced > Standard khÃ´ng thá»a mÃ£n
**NguyÃªn nhÃ¢n**: Logic tÃ­nh toÃ¡n hoáº·c grouping khÃ´ng Ä‘Ãºng

**Giáº£i phÃ¡p**:
- Kiá»ƒm tra class_to_group mapping
- Verify per-group error computation

---

## ğŸ¨ Customization

### Thay Ä‘á»•i coverage range:
```python
config['coverage_min'] = 0.0  # Start from 0%
config['coverage_max'] = 1.0  # Go to 100%
config['num_points'] = 101    # 101 points
```

### ThÃªm paper baseline:
```python
# Trong eval_paper_benchmark.py
def _load_paper_results(self):
    return {
        'selectivenet': {
            'cifar100': {
                'aurc_0.2_1.0': 0.12,  # Fill with actual values
                'accuracy_at_90': 0.85,
            }
        }
    }
```

### Custom visualization:
```python
# Modify plot_paper_style_figures() trong eval_paper_benchmark.py
def plot_paper_style_figures(self, results: Dict):
    # Your custom plotting code
    pass
```

---

## ğŸ“š References

1. **SelectiveNet**: Geifman & El-Yaniv, "Selective Classification for Deep Neural Networks", ICML 2019
2. **Learning to Reject Meets Long-tail Learning**: Cao et al., 2024
3. **Rejection Option**: Chow, "On Optimum Recognition Error and Reject Tradeoff", IEEE Trans. 1970

---

## ğŸ’¡ Tips

### Äá»ƒ cÃ³ káº¿t quáº£ tá»‘t nháº¥t:

1. **Train nhiá»u checkpoints** vÃ  chá»n checkpoint tá»‘t nháº¥t dá»±a trÃªn validation AURC
2. **Thá»­ nhiá»u confidence methods** Ä‘á»ƒ tÃ¬m phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p nháº¥t
3. **So sÃ¡nh vá»›i Oracle** Ä‘á»ƒ biáº¿t upper bound
4. **Kiá»ƒm tra per-group fairness** (Head vs Tail)
5. **Bootstrap confidence intervals** náº¿u muá»‘n statistical significance

### Äá»ƒ paper Ä‘Æ°á»£c accept:

1. **BÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§**: Standard, Balanced, Worst-Group metrics
2. **So sÃ¡nh vá»›i baselines**: SelectiveNet, Random, Oracle
3. **Confidence intervals**: Report mean Â± std over multiple runs
4. **Ablation studies**: Analyze Î±, Î¼ impact
5. **Qualitative analysis**: Show sample rejection examples

---

## âœ¨ Example Results

VÃ­ dá»¥ output console:

```
================================================================================
PAPER BENCHMARK EVALUATION
Comparison with SelectiveNet & Learning to Reject Meets Long-tail Learning
================================================================================

âœ… Initialized evaluator with 10000 test samples
âœ… Loaded model with Î±=[0.9234, 1.1567], Î¼=[0.4123, 0.5234]

ğŸ”„ Computing confidence scores using: gse_margin
ğŸ”„ Generating Risk-Coverage curve...
ğŸ”„ Computing selective risk metrics...
ğŸ”„ Evaluating at specific coverage points...
ğŸ”„ Computing oracle baseline...

================================================================================
EVALUATION SUMMARY
================================================================================

ğŸ“Š AURC Metrics (Coverage 0.2-1.0):
  Standard Error:
    AURC:    0.123456
    E-AURC:  -0.012345

  Balanced Error:
    AURC:    0.234567
    E-AURC:  -0.023456

  Worst-Group Error:
    AURC:    0.345678
    E-AURC:  -0.034567

ğŸ“Š Oracle Baseline:
  Standard:    0.056789
  Balanced:    0.067890
  Worst-Group: 0.078901

ğŸ“Š Metrics at Specific Coverages:

  Coverage â‰ˆ 0.601 (target 0.60):
    Standard Accuracy:    0.8456
    Balanced Accuracy:    0.7890
    Worst-Group Accuracy: 0.7234

================================================================================

ğŸ“Š Saved paper-style figures to ./paper_benchmark_results
ğŸ“ Saved LaTeX table to ./paper_benchmark_results/latex_table.tex
ğŸ’¾ Saved detailed results to ./paper_benchmark_results

================================================================================
âœ… PAPER BENCHMARK EVALUATION COMPLETE
================================================================================
```

---

## ğŸ¤ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra log messages Ä‘á»ƒ xÃ¡c Ä‘á»‹nh lá»—i
2. Verify input files (logits, labels, checkpoint)
3. Thá»­ vá»›i confidence method Ä‘Æ¡n giáº£n hÆ¡n (`max_posterior`)
4. Giáº£m `num_points` náº¿u cháº¡y quÃ¡ lÃ¢u

Good luck vá»›i paper submission! ğŸ‰
