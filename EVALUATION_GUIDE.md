# EVALUATION_GUIDE.md

# H∆∞·ªõng D·∫´n Evaluation So S√°nh v·ªõi Paper Benchmarks

T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n c√°ch ƒë√°nh gi√° m√¥ h√¨nh AR-GSE v√† so s√°nh v·ªõi c√°c paper nh∆∞ **SelectiveNet** v√† **Learning to Reject Meets Long-tail Learning**.

---

## üìã T·ªïng Quan

Ch√∫ng t√¥i cung c·∫•p 3 script evaluation:

1. **`eval_comprehensive.py`** - Evaluation to√†n di·ªán v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p t√≠nh confidence
2. **`eval_paper_benchmark.py`** - So s√°nh tr·ª±c ti·∫øp v·ªõi paper benchmarks  
3. **`run_benchmark_evaluation.py`** - Script nhanh ƒë·ªÉ ch·∫°y evaluation

---

## üéØ Metrics Ch√≠nh

### 1. AURC (Area Under Risk-Coverage Curve)
- **ƒê·ªãnh nghƒ©a**: Di·ªán t√≠ch d∆∞·ªõi ƒë∆∞·ªùng cong Risk-Coverage
- **C√¥ng th·ª©c**: $\text{AURC} = \int_{c_{min}}^{c_{max}} \text{Risk}(c) \, dc$
- **T·ªët h∆°n khi**: Th·∫•p h∆°n (lower is better)
- **Range**: [0, 1]

### 2. E-AURC (Excess AURC)
- **ƒê·ªãnh nghƒ©a**: AURC v∆∞·ª£t qu√° baseline ng·∫´u nhi√™n
- **C√¥ng th·ª©c**: $\text{E-AURC} = \text{AURC}_{\text{method}} - \text{AURC}_{\text{random}}$
- **T·ªët h∆°n khi**: √Çm (negative is better)

### 3. Coverage-Error Trade-offs
- ƒê√°nh gi√° hi·ªáu su·∫•t t·∫°i c√°c m·ª©c coverage c·ª• th·ªÉ: 60%, 70%, 80%, 90%, 95%
- So s√°nh Standard Error, Balanced Error, Worst-Group Error

---

## üöÄ C√°ch S·ª≠ D·ª•ng

### Ph∆∞∆°ng √°n 1: Script nhanh (Khuy·∫øn ngh·ªã)

```bash
# Evaluation c∆° b·∫£n
python run_benchmark_evaluation.py

# V·ªõi custom checkpoint
python run_benchmark_evaluation.py \
    --checkpoint ./checkpoints/my_model/gse_plugin.ckpt \
    --output ./my_results

# Ch·ªâ ƒë·ªãnh dataset v√† experts
python run_benchmark_evaluation.py \
    --checkpoint ./checkpoints/argse_balanced/cifar100_lt_if100/gse_plugin.ckpt \
    --dataset cifar100_lt_if100 \
    --experts ce_baseline logitadjust_baseline balsoftmax_baseline \
    --output ./benchmark_results_balanced
```

### Ph∆∞∆°ng √°n 2: Python script chi ti·∫øt

```python
from src.train.eval_paper_benchmark import PaperBenchmarkEvaluator

config = {
    'dataset': {
        'name': 'cifar100_lt_if100',
        'splits_dir': './data/cifar100_lt_if100_splits',
        'num_classes': 100,
    },
    'experts': {
        'names': ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline'],
        'logits_dir': './outputs/logits',
    },
    'checkpoint_path': './checkpoints/argse_worst_eg_improved/cifar100_lt_if100/gse_balanced_plugin.ckpt',
    'output_dir': './paper_benchmark_results',
    'seed': 42
}

evaluator = PaperBenchmarkEvaluator(config)
results = evaluator.run_paper_benchmark()
```

### Ph∆∞∆°ng √°n 3: Comprehensive evaluation (so s√°nh nhi·ªÅu methods)

```python
from src.train.eval_comprehensive import ComprehensiveEvaluator

config = {
    'dataset': {
        'name': 'cifar100_lt_if100',
        'splits_dir': './data/cifar100_lt_if100_splits',
        'num_classes': 100,
    },
    'experts': {
        'names': ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline'],
        'logits_dir': './outputs/logits',
    },
    'checkpoint_path': './checkpoints/argse_worst_eg_improved/cifar100_lt_if100/gse_balanced_plugin.ckpt',
    'output_dir': './comprehensive_evaluation_results',
    'coverage_min': 0.2,
    'coverage_max': 1.0,
    'num_points': 81,
    'seed': 42
}

evaluator = ComprehensiveEvaluator(config)
results = evaluator.run_full_evaluation()
```

---

## üìä Output Files

Sau khi ch·∫°y evaluation, b·∫°n s·∫Ω c√≥ c√°c file:

### 1. `paper_benchmark_results.json`
```json
{
    "dataset": "cifar100_lt_if100",
    "num_test_samples": 10000,
    "method": "gse_margin",
    "aurc_metrics": {
        "standard": {
            "aurc": 0.123456,
            "eaurc": -0.012345
        },
        "balanced": {
            "aurc": 0.234567,
            "eaurc": -0.023456
        },
        "worst": {
            "aurc": 0.345678,
            "eaurc": -0.034567
        }
    },
    "oracle": {
        "aurc_standard": 0.056789,
        "aurc_balanced": 0.067890,
        "aurc_worst": 0.078901
    },
    "coverage_metrics": {
        "cov_0.60": {
            "standard_accuracy": 0.85,
            "balanced_accuracy": 0.82,
            "worst_accuracy": 0.78
        }
    }
}
```

### 2. `paper_benchmark_figures.png/pdf`
6-panel figure ch·∫•t l∆∞·ª£ng publication:
- (a) Standard Error RC Curve
- (b) Balanced Error RC Curve  
- (c) Worst-Group Error RC Curve
- (d) Per-Group Selective Errors
- (e) AURC Metrics Comparison
- (f) Coverage-Accuracy Trade-off

### 3. `latex_table.tex`
B·∫£ng LaTeX s·∫µn s√†ng cho paper submission:
```latex
\begin{table}[t]
\centering
\caption{Selective Classification Performance on CIFAR-100-LT}
\label{tab:main_results}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{AURC-Std} & \textbf{AURC-Bal} & \textbf{AURC-Worst} \\
\midrule
AR-GSE (Ours) & 0.1234 & 0.2345 & \textbf{0.3456} \\
...
\end{tabular}
\end{table}
```

### 4. `rc_curve_paper_benchmark.csv`
Raw data c·ªßa RC curve ƒë·ªÉ v·∫Ω l·∫°i ho·∫∑c ph√¢n t√≠ch th√™m

---

## üî¨ Confidence Scoring Methods

Framework h·ªó tr·ª£ nhi·ªÅu ph∆∞∆°ng ph√°p t√≠nh confidence score:

### 1. GSE Margin (Khuy·∫øn ngh·ªã)
```python
confidence = compute_margin(eta_mix, alpha, mu, c=0.0, class_to_group)
```
- S·ª≠ d·ª•ng GSE decision function: $\text{score} - \text{threshold}$
- Ph√π h·ª£p nh·∫•t v·ªõi AR-GSE framework

### 2. GSE Raw Margin
```python
confidence = compute_raw_margin(eta_mix, alpha, mu, class_to_group)
```
- Kh√¥ng tr·ª´ rejection cost $c$

### 3. Maximum Posterior
```python
confidence = max_y P(y|x)
```
- Standard baseline t·ª´ SelectiveNet

### 4. Entropy-based
```python
confidence = -H(P(y|x)) = -Œ£_y P(y|x) log P(y|x)
```
- Negative entropy l√†m confidence score

### 5. Margin (Top-2 difference)
```python
confidence = P(y_1|x) - P(y_2|x)
```
- Hi·ªáu s·ªë gi·ªØa 2 x√°c su·∫•t cao nh·∫•t

---

## üìà C√°ch Hi·ªÉu K·∫øt Qu·∫£

### ‚úÖ K·∫øt qu·∫£ t·ªët:
- **AURC th·∫•p** (< 0.3 l√† t·ªët, < 0.2 l√† r·∫•t t·ªët)
- **E-AURC √¢m** (c√†ng √¢m c√†ng t·ªët so v·ªõi random)
- **AURC g·∫ßn Oracle** (< 2x Oracle l√† acceptable)
- **Worst-Group Error kh√¥ng qu√° cao** (< 2x Balanced Error)

### ‚ö†Ô∏è C·∫£nh b√°o:
- AURC > 0.5: Model kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨
- E-AURC > 0: T·ªá h∆°n random rejection
- Worst-Group Error >> Balanced Error: Thi√™n l·ªách nghi√™m tr·ªçng

### üìä So s√°nh v·ªõi Papers:

#### SelectiveNet (ICML 2019)
- **CIFAR-10**: AURC ‚âà 0.02-0.03
- **CIFAR-100**: AURC ‚âà 0.10-0.15
- **ImageNet**: AURC ‚âà 0.05-0.08

#### Learning to Reject Meets Long-tail Learning (2024)
- **CIFAR-100-LT**: 
  - Balanced AURC ‚âà 0.15-0.20
  - Worst-Group AURC ‚âà 0.25-0.35
  - Coverage 0.7: Balanced Acc ‚âà 0.75-0.80

---

## üîç Debug v√† Troubleshooting

### L·ªói th∆∞·ªùng g·∫∑p:

#### 1. File not found
```
FileNotFoundError: Logits file not found
```
**Gi·∫£i ph√°p**: Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n logits:
```bash
ls -la outputs/logits/cifar100_lt_if100/*/test_lt_logits.pt
```

#### 2. AURC qu√° cao (> 0.5)
**Nguy√™n nh√¢n**: Model ch∆∞a converge ho·∫∑c confidence scores kh√¥ng h·ª£p l√Ω

**Gi·∫£i ph√°p**:
- Ki·ªÉm tra confidence score distribution:
```python
import torch
confidence = evaluator.compute_confidence_scores('gse_margin')
print(f"Min: {confidence.min():.3f}, Max: {confidence.max():.3f}")
print(f"Mean: {confidence.mean():.3f}, Std: {confidence.std():.3f}")
```
- Th·ª≠ ph∆∞∆°ng ph√°p kh√°c: `max_posterior`, `entropy`

#### 3. Worst > Balanced > Standard kh√¥ng th·ªèa m√£n
**Nguy√™n nh√¢n**: Logic t√≠nh to√°n ho·∫∑c grouping kh√¥ng ƒë√∫ng

**Gi·∫£i ph√°p**:
- Ki·ªÉm tra class_to_group mapping
- Verify per-group error computation

---

## üé® Customization

### Thay ƒë·ªïi coverage range:
```python
config['coverage_min'] = 0.0  # Start from 0%
config['coverage_max'] = 1.0  # Go to 100%
config['num_points'] = 101    # 101 points
```

### Th√™m paper baseline:
```python
# Trong eval_paper_benchmark.py
def _load_paper_results(self):
    return {
        'selectivenet': {
            'cifar100': {
                'aurc_0.2_1.0': 0.12,  # Fill with actual values
                'accuracy_at_90': 0.85,
            }
        }
    }
```

### Custom visualization:
```python
# Modify plot_paper_style_figures() trong eval_paper_benchmark.py
def plot_paper_style_figures(self, results: Dict):
    # Your custom plotting code
    pass
```

---

## üìö References

1. **SelectiveNet**: Geifman & El-Yaniv, "Selective Classification for Deep Neural Networks", ICML 2019
2. **Learning to Reject Meets Long-tail Learning**: Cao et al., 2024
3. **Rejection Option**: Chow, "On Optimum Recognition Error and Reject Tradeoff", IEEE Trans. 1970

---

## üí° Tips

### ƒê·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t:

1. **Train nhi·ªÅu checkpoints** v√† ch·ªçn checkpoint t·ªët nh·∫•t d·ª±a tr√™n validation AURC
2. **Th·ª≠ nhi·ªÅu confidence methods** ƒë·ªÉ t√¨m ph∆∞∆°ng ph√°p ph√π h·ª£p nh·∫•t
3. **So s√°nh v·ªõi Oracle** ƒë·ªÉ bi·∫øt upper bound
4. **Ki·ªÉm tra per-group fairness** (Head vs Tail)
5. **Bootstrap confidence intervals** n·∫øu mu·ªën statistical significance

### ƒê·ªÉ paper ƒë∆∞·ª£c accept:

1. **B√°o c√°o ƒë·∫ßy ƒë·ªß**: Standard, Balanced, Worst-Group metrics
2. **So s√°nh v·ªõi baselines**: SelectiveNet, Random, Oracle
3. **Confidence intervals**: Report mean ¬± std over multiple runs
4. **Ablation studies**: Analyze Œ±, Œº impact
5. **Qualitative analysis**: Show sample rejection examples

---

## ‚ú® Example Results

V√≠ d·ª• output console:

```
================================================================================
PAPER BENCHMARK EVALUATION
Comparison with SelectiveNet & Learning to Reject Meets Long-tail Learning
================================================================================

‚úÖ Initialized evaluator with 10000 test samples
‚úÖ Loaded model with Œ±=[0.9234, 1.1567], Œº=[0.4123, 0.5234]

üîÑ Computing confidence scores using: gse_margin
üîÑ Generating Risk-Coverage curve...
üîÑ Computing selective risk metrics...
üîÑ Evaluating at specific coverage points...
üîÑ Computing oracle baseline...

================================================================================
EVALUATION SUMMARY
================================================================================

üìä AURC Metrics (Coverage 0.2-1.0):
  Standard Error:
    AURC:    0.123456
    E-AURC:  -0.012345

  Balanced Error:
    AURC:    0.234567
    E-AURC:  -0.023456

  Worst-Group Error:
    AURC:    0.345678
    E-AURC:  -0.034567

üìä Oracle Baseline:
  Standard:    0.056789
  Balanced:    0.067890
  Worst-Group: 0.078901

üìä Metrics at Specific Coverages:

  Coverage ‚âà 0.601 (target 0.60):
    Standard Accuracy:    0.8456
    Balanced Accuracy:    0.7890
    Worst-Group Accuracy: 0.7234

================================================================================

üìä Saved paper-style figures to ./paper_benchmark_results
üìù Saved LaTeX table to ./paper_benchmark_results/latex_table.tex
üíæ Saved detailed results to ./paper_benchmark_results

================================================================================
‚úÖ PAPER BENCHMARK EVALUATION COMPLETE
================================================================================
```

---

## ü§ù Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ:
1. Ki·ªÉm tra log messages ƒë·ªÉ x√°c ƒë·ªãnh l·ªói
2. Verify input files (logits, labels, checkpoint)
3. Th·ª≠ v·ªõi confidence method ƒë∆°n gi·∫£n h∆°n (`max_posterior`)
4. Gi·∫£m `num_points` n·∫øu ch·∫°y qu√° l√¢u

Good luck v·ªõi paper submission! üéâ
